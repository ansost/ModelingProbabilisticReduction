{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import buckeye\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dp.phonemizer import Phonemizer\n",
    "\n",
    "phonemizer = Phonemizer.from_checkpoint(\"/home/anna/anna/data/en_us_cmudict_forward.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26529cde",
   "metadata": {},
   "source": [
    "### Global Speech Rate\n",
    "Defined as number of syllables per utterance divided by the total duration of the utterance.\n",
    "\n",
    "Presupposes a dataframe with pauses, words and two empty columns: 'utteranceID' and 'global_sr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = buckeye.corpus(\n",
    "    \"/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/buckeye_data/zipped_corpus/\"\n",
    ")\n",
    "df = pd.DataFrame({\"items\": [], \"trackID\": []})\n",
    "num = 0\n",
    "\n",
    "for speaker in tqdm(corpus):\n",
    "    num = num + 1\n",
    "    tracks = {}\n",
    "    trackNum = 0\n",
    "    for track in speaker:\n",
    "        trackWords = []\n",
    "\n",
    "        for word in track.words:\n",
    "            if isinstance(word, buckeye.containers.Word):\n",
    "                trackWords.append(word)\n",
    "            elif isinstance(word, buckeye.containers.Pause):\n",
    "                pauseType = str(word).split(\" \")\n",
    "                trackWords.append(pauseType[1])\n",
    "\n",
    "        tracks[trackNum] = trackWords\n",
    "\n",
    "        trackNum = trackNum + 1\n",
    "\n",
    "    for entry in tracks.items():\n",
    "        for word in entry[1]:\n",
    "            df.loc[len(df)] = {\"items\": word, \"trackID\": entry[0]}\n",
    "\n",
    "    print(df)\n",
    "    # df.to_csv('/home/anna/anna/data/temp/s' + str(num) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a666b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = buckeye.corpus(\n",
    "    \"/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/buckeye_data/zipped_corpus/\"\n",
    ")\n",
    "for speaker in corpus:\n",
    "    for track in speaker:\n",
    "        for word in track.words:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a230b",
   "metadata": {},
   "source": [
    "### working version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = buckeye.corpus(\n",
    "    \"/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/buckeye_data/zipped_corpus/\"\n",
    ")\n",
    "num = 0\n",
    "pattern = r\"\\{(\\w*)\\}|\\<(\\w*)\\>\"\n",
    "utteranceID = 1\n",
    "inUtterance = bool\n",
    "forbidden_words = [\n",
    "    \"oh\",\n",
    "    \"uh\",\n",
    "    \"ah\",\n",
    "    \"um\",\n",
    "    \"mm\",\n",
    "    \"hm\",\n",
    "    \"huh\",\n",
    "    \"uh-huh\",\n",
    "    \"um-hum\",\n",
    "    \"huh-uh\",\n",
    "    \"hum-hum\",\n",
    "    \"hmm\",\n",
    "    \"hmmm\",\n",
    "    \"mh\",\n",
    "    \"mmh\",\n",
    "]\n",
    "# Appends all speaker dfs so there are no speaker overlaps while the df is being constructed.\n",
    "all_dfs = []\n",
    "\n",
    "for speaker in tqdm(corpus):\n",
    "    num = num + 1\n",
    "    df_name = str(num) + \"df\"\n",
    "    df_name = pd.DataFrame({\"items\": [], \"utteranceID\": [], \"global_sr\": []})\n",
    "    speaker_words = []\n",
    "\n",
    "    for track in speaker:\n",
    "        for word in track.words:\n",
    "            if isinstance(word, buckeye.containers.Word):\n",
    "                speaker_words.append(word)\n",
    "            elif isinstance(word, buckeye.containers.Pause):\n",
    "                pauseType = str(word).split(\" \")\n",
    "                speaker_words.append(pauseType[1])\n",
    "\n",
    "    # Collects words per utterance\n",
    "    wordsUtterance = []\n",
    "\n",
    "    for index, word in enumerate(speaker_words):\n",
    "\n",
    "        if (\n",
    "            isinstance(word, buckeye.containers.Word)\n",
    "            and word.orthography not in forbidden_words\n",
    "        ):\n",
    "            inUtterance = True\n",
    "        else:\n",
    "            inUtterance = False\n",
    "\n",
    "        if inUtterance == True:\n",
    "            raw_syllables = stringify(\n",
    "                syllabify(English, get_segments(word.orthography, upper=True))\n",
    "            )\n",
    "            wordsUtterance.append(\n",
    "                (index, word.dur, len(raw_syllables.split(\" \")), word.orthography)\n",
    "            )\n",
    "\n",
    "        elif inUtterance == False and len(wordsUtterance) > 0:\n",
    "            totalDur = sum([item[1] for item in wordsUtterance])\n",
    "            totalSyl = sum([item[2] for item in wordsUtterance])\n",
    "            globalSr = totalSyl / totalDur\n",
    "\n",
    "            for entry in wordsUtterance:\n",
    "                df_name.at[entry[0], \"utteranceID\"] = utteranceID\n",
    "                df_name.at[entry[0], \"global_sr\"] = globalSr\n",
    "                df_name.at[entry[0], \"items\"] = entry[3]\n",
    "\n",
    "            utteranceID = utteranceID + 1\n",
    "            wordsUtterance = []\n",
    "\n",
    "    ## Sanity Test\n",
    "    sprecher = pd.read_csv(\n",
    "        \"/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/buckeye_data/allwords_perspeaker_csv/s\"\n",
    "        + str(num)\n",
    "        + \".csv\"\n",
    "    )\n",
    "    items = df_name[\"items\"].tolist()\n",
    "    tokens = sprecher[\"token\"].tolist()\n",
    "    assert len(tokens) == len(items)\n",
    "    del sprecher, items, tokens\n",
    "\n",
    "    df_name.reset_index(drop=True, inplace=True)\n",
    "    all_dfs.append(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_dfs) == 40\n",
    "\n",
    "alle = pd.concat(all_dfs)\n",
    "alle.to_csv(\"/home/anna/anna/data/globalSr_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfa829",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = pd.read_csv(\n",
    "    \"/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/R/BA/data/regression_data_otherPrior.csv\",\n",
    "    low_memory=True,\n",
    "    engine=\"c\",\n",
    "    dtype={\n",
    "        \"speakerID\": \"category\",\n",
    "        \"speakerAge\": \"category\",\n",
    "        \"speakerGender\": \"category\",\n",
    "        \"interviewerGender\": \"category\",\n",
    "        \"wordPOS\": \"category\",\n",
    "        \"n_segments\": \"category\",\n",
    "        \"n_syllables\": \"category\",\n",
    "    },\n",
    ")\n",
    "alle_dfs = pd.concat(all_dfs, axis=0)\n",
    "alle_dfs.reset_index(drop=True, inplace=True)\n",
    "finalDF = pd.concat([regression, alle_dfs], axis=1)\n",
    "finalDF.to_csv(\n",
    "    \"/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/R/BA/data/regression_data.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalfinalDF = finalDF.drop(\"items\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalfinalDF.to_csv(\n",
    "    \"/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/R/BA/data/regression_data.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b63ce",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting/formatting segments/syllables\n",
    "def get_segments(word, upper=False):\n",
    "    \"\"\"Returns the segments of a word.\"\"\"\n",
    "    raw_segment_string = phonemizer(str(word), lang=\"en_us\")\n",
    "\n",
    "    if upper:\n",
    "        segments_string = re.sub(r\"[\\[\\]-]\", \" \", raw_segment_string)\n",
    "        segments = segments_string.split()\n",
    "        return segments\n",
    "    else:\n",
    "        segments_string = re.sub(r\"[\\[\\]-]\", \" \", raw_segment_string.lower())\n",
    "        segments = segments_string.split()\n",
    "        return segments\n",
    "\n",
    "\n",
    "def join_segments(word):\n",
    "    \"\"\"Returns the segments of a word in a cue formatted string.\"\"\"\n",
    "    segments = get_segments(word)\n",
    "    segments_y = []\n",
    "    for segment in segments:\n",
    "        segment = \"s.\" + segment\n",
    "        segments_y.append(segment)\n",
    "    segments_joined = \"_\".join(segments_y)\n",
    "    return segments_joined\n",
    "\n",
    "\n",
    "def join_syllables(syllables):\n",
    "    \"\"\"Returns the syllables of a word in a cue formatted string.\"\"\"\n",
    "    syll_list = syllables.split()\n",
    "    syllable_cuestring = []\n",
    "    for entry in syll_list:\n",
    "        syllable_cue = \"y.\" + entry\n",
    "        syllable_cuestring.append(syllable_cue.lower())\n",
    "    syllables_joined = \"_\".join(syllable_cuestring)\n",
    "    return syllables_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Syllabifier script.\n",
    "# English language settings for the language parameter in the syllabifier.\n",
    "English = {\n",
    "    \"consonants\": [\n",
    "        \"B\",\n",
    "        \"CH\",\n",
    "        \"D\",\n",
    "        \"DH\",\n",
    "        \"F\",\n",
    "        \"G\",\n",
    "        \"HH\",\n",
    "        \"JH\",\n",
    "        \"K\",\n",
    "        \"L\",\n",
    "        \"M\",\n",
    "        \"N\",\n",
    "        \"NG\",\n",
    "        \"P\",\n",
    "        \"R\",\n",
    "        \"S\",\n",
    "        \"SH\",\n",
    "        \"T\",\n",
    "        \"TH\",\n",
    "        \"V\",\n",
    "        \"W\",\n",
    "        \"Y\",\n",
    "        \"Z\",\n",
    "        \"ZH\",\n",
    "    ],\n",
    "    \"vowels\": [\n",
    "        \"AA\",\n",
    "        \"AE\",\n",
    "        \"AH\",\n",
    "        \"AO\",\n",
    "        \"AW\",\n",
    "        \"AY\",\n",
    "        \"EH\",\n",
    "        \"ER\",\n",
    "        \"EY\",\n",
    "        \"IH\",\n",
    "        \"IY\",\n",
    "        \"OW\",\n",
    "        \"OY\",\n",
    "        \"UH\",\n",
    "        \"UW\",\n",
    "    ],\n",
    "    \"onsets\": [\n",
    "        \"P\",\n",
    "        \"T\",\n",
    "        \"K\",\n",
    "        \"B\",\n",
    "        \"D\",\n",
    "        \"G\",\n",
    "        \"F\",\n",
    "        \"V\",\n",
    "        \"TH\",\n",
    "        \"DH\",\n",
    "        \"S\",\n",
    "        \"Z\",\n",
    "        \"SH\",\n",
    "        \"CH\",\n",
    "        \"JH\",\n",
    "        \"M\",\n",
    "        \"N\",\n",
    "        \"R\",\n",
    "        \"L\",\n",
    "        \"HH\",\n",
    "        \"W\",\n",
    "        \"Y\",\n",
    "        \"P R\",\n",
    "        \"T R\",\n",
    "        \"K R\",\n",
    "        \"B R\",\n",
    "        \"D R\",\n",
    "        \"G R\",\n",
    "        \"F R\",\n",
    "        \"TH R\",\n",
    "        \"SH R\",\n",
    "        \"P L\",\n",
    "        \"K L\",\n",
    "        \"B L\",\n",
    "        \"G L\",\n",
    "        \"F L\",\n",
    "        \"S L\",\n",
    "        \"T W\",\n",
    "        \"K W\",\n",
    "        \"D W\",\n",
    "        \"S W\",\n",
    "        \"S P\",\n",
    "        \"S T\",\n",
    "        \"S K\",\n",
    "        \"S F\",\n",
    "        \"S M\",\n",
    "        \"S N\",\n",
    "        \"G W\",\n",
    "        \"SH W\",\n",
    "        \"S P R\",\n",
    "        \"S P L\",\n",
    "        \"S T R\",\n",
    "        \"S K R\",\n",
    "        \"S K W\",\n",
    "        \"S K L\",\n",
    "        \"TH W\",\n",
    "        \"ZH\",\n",
    "        \"P Y\",\n",
    "        \"K Y\",\n",
    "        \"B Y\",\n",
    "        \"F Y\",\n",
    "        \"HH Y\",\n",
    "        \"V Y\",\n",
    "        \"TH Y\",\n",
    "        \"M Y\",\n",
    "        \"S P Y\",\n",
    "        \"S K Y\",\n",
    "        \"G Y\",\n",
    "        \"HH W\",\n",
    "        \"\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def syllabify(language, word):\n",
    "    \"\"\"Syllabifies the word, given a language configuration loaded with\n",
    "    loadLanguage. word is either a string of phonemes from the CMU\n",
    "    pronouncing dictionary set (with optional stress numbers after vowels),\n",
    "    or a Python list of phonemes, e.g. \"B AE1 T\" or [\"B\", \"AE1\", \"T\"]\n",
    "    \"\"\"\n",
    "\n",
    "    if type(word) == str:\n",
    "        word = word.split()\n",
    "    # This is the returned data structure.\n",
    "    syllables = []\n",
    "\n",
    "    # This maintains a list of phonemes between nuclei.\n",
    "    internuclei = []\n",
    "\n",
    "    for phoneme in word:\n",
    "\n",
    "        phoneme = phoneme.strip()\n",
    "        if phoneme == \"\":\n",
    "            continue\n",
    "        stress = None\n",
    "        if phoneme[-1].isdigit():\n",
    "            stress = int(phoneme[-1])\n",
    "            phoneme = phoneme[0:-1]\n",
    "\n",
    "        # Split the consonants seen since the last nucleus into coda and\n",
    "        # onset.\n",
    "        if phoneme in language[\"vowels\"]:\n",
    "\n",
    "            coda = None\n",
    "            onset = None\n",
    "\n",
    "            # If there is a period in the input, split there.\n",
    "            if \".\" in internuclei:\n",
    "                period = internuclei.index(\".\")\n",
    "                coda = internuclei[:period]\n",
    "                onset = internuclei[period + 1 :]\n",
    "\n",
    "            else:\n",
    "                # Make the largest onset we can. The 'split' variable marks\n",
    "                # the break point.\n",
    "                for split in range(0, len(internuclei) + 1):\n",
    "                    coda = internuclei[:split]\n",
    "                    onset = internuclei[split:]\n",
    "\n",
    "                    # If we are looking at a valid onset, or if we're at the\n",
    "                    # start of the word (in which case an invalid onset is\n",
    "                    # better than a coda that doesn't follow a nucleus), or\n",
    "                    # if we've gone through all of the onsets and we didn't\n",
    "                    # find any that are valid, then split the nonvowels\n",
    "                    # we've seen at this location.\n",
    "                    if (\n",
    "                        \" \".join(onset) in language[\"onsets\"]\n",
    "                        or len(syllables) == 0\n",
    "                        or len(onset) == 0\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "            # Tack the coda onto the coda of the last syllable. Can't do it\n",
    "            # if this is the first syllable.\n",
    "            if len(syllables) > 0:\n",
    "                syllables[-1][3].extend(coda)\n",
    "\n",
    "            # Make a new syllable out of the onset and nucleus.\n",
    "            syllables.append((stress, onset, [phoneme], []))\n",
    "\n",
    "            # At this point we've processed the internuclei list.\n",
    "            internuclei = []\n",
    "\n",
    "        elif not phoneme in language[\"consonants\"] and phoneme != \".\":\n",
    "            raise ValueError(\"Invalid phoneme: \" + phoneme)\n",
    "\n",
    "        else:  # a consonant\n",
    "            internuclei.append(phoneme)\n",
    "\n",
    "    # Done looping through phonemes. We may have consonants left at the end.\n",
    "    # We may have even not found a nucleus.\n",
    "    if len(internuclei) > 0:\n",
    "        if len(syllables) == 0:\n",
    "            syllables.append((None, internuclei, [], []))\n",
    "        else:\n",
    "            syllables[-1][3].extend(internuclei)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "\n",
    "def stringify(syllables):\n",
    "    \"\"\"This function takes a syllabification returned by syllabify and\n",
    "    turns it into a string, with phonemes spearated by spaces and\n",
    "    syllables spearated by periods.\"\"\"\n",
    "    ret = []\n",
    "    for syl in syllables:\n",
    "        stress, onset, nucleus, coda = syl\n",
    "        if stress != None and len(nucleus) != 0:\n",
    "            nucleus[0] += str(stress)\n",
    "        ret.append(\"\".join(onset + nucleus + coda))\n",
    "    return \" \".join(ret)\n",
    "\n",
    "\n",
    "language = English"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
