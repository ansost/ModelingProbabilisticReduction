{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7ebbc9",
   "metadata": {},
   "source": [
    "## Making event files from the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4e21a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dp.phonemizer import Phonemizer\n",
    "\n",
    "phonemizer = Phonemizer.from_checkpoint(\n",
    "    \"/gpfs/project/anste145/en_us_cmudict_forward.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496ea97",
   "metadata": {
    "code_folding": [
     0,
     11,
     24,
     34
    ]
   },
   "outputs": [],
   "source": [
    "def get_context(index, word):\n",
    "    if index == 0:\n",
    "        context = \"c.\" + words[index + 1]\n",
    "    elif index == len(words) - 1:\n",
    "        context = \"c.\" + words[index - 1]\n",
    "    else:\n",
    "        previous_word = \"c.\" + words[index - 1]\n",
    "        following_word = \"c.\" + words[index + 1]\n",
    "        context = previous_word + \"_\" + following_word\n",
    "    return context\n",
    "\n",
    "\n",
    "def get_segments(word, upper=False):\n",
    "    \"\"\"Returns the segments of a word.\"\"\"\n",
    "    raw_segment_string = phonemizer(word, lang=\"en_us\")\n",
    "\n",
    "    if upper:\n",
    "        segments_string = re.sub(r\"[\\[\\]-]\", \" \", raw_segment_string)\n",
    "        # segments = segments_string.split()\n",
    "        return segments_string\n",
    "    else:\n",
    "        segments_string = re.sub(r\"[\\[\\]-]\", \" \", raw_segment_string.lower())\n",
    "        segments = segments_string.split()\n",
    "        return segments\n",
    "\n",
    "\n",
    "def join_segments(word):\n",
    "    \"\"\"Returns the segments of a word in a cue formatted string.\"\"\"\n",
    "    segments = get_segments(word)\n",
    "    segments_y = []\n",
    "    for segment in segments:\n",
    "        segment = \"s.\" + segment\n",
    "        segments_y.append(segment)\n",
    "    segments_joined = \"_\".join(segments_y)\n",
    "    return segments_joined\n",
    "\n",
    "\n",
    "def join_syllables(syllables):\n",
    "    \"\"\"Returns the syllables of a word in a cue formatted string.\"\"\"\n",
    "    syll_list = syllables.split()\n",
    "    syllable_cuestring = []\n",
    "    for entry in syll_list:\n",
    "        syllable_cue = \"y.\" + entry\n",
    "        syllable_cuestring.append(syllable_cue)\n",
    "    syllables_joined = \"_\".join(syllable_cuestring)\n",
    "    return syllables_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb0464",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/gpfs/project/anste145/input_files/buckeye_data/allwords_perspeaker_csv/\"\n",
    "files = os.listdir(path)\n",
    "\n",
    "speakers = []\n",
    "transcriptions = {}\n",
    "\n",
    "for file in tqdm(files):\n",
    "\n",
    "    print(\"We're at \", file)\n",
    "\n",
    "    # List of words for the speaker\n",
    "    df = pd.read_csv(path + file)\n",
    "    words = df[\"token\"].tolist()\n",
    "\n",
    "    # Create new dataframe for speaker.\n",
    "    df_name = file.replace(\".csv\", \"\")\n",
    "    df = pd.DataFrame({\"cues\": [], \"outcomes\": []})\n",
    "\n",
    "    for index, word in enumerate(words):\n",
    "\n",
    "        # Get context.\n",
    "        context = get_context(index, word)\n",
    "\n",
    "        # Get Segments.\n",
    "        if word not in transcriptions.keys():\n",
    "            segments = join_segments(word)\n",
    "            raw_segments = get_segments(word, upper=True)\n",
    "            transcriptions[word] = {\n",
    "                \"cue_segments\": str(segments),\n",
    "                \"segments\": str(raw_segments),\n",
    "            }\n",
    "        else:\n",
    "            segments = transcriptions[word][\"cue_segments\"]\n",
    "\n",
    "        # Get syllables.\n",
    "        raw_syllables = stringify(syllabify(English, transcriptions[word][\"segments\"]))\n",
    "        syllables = join_syllables(raw_syllables)\n",
    "\n",
    "        # Append all cue strings and clean track boundaries\n",
    "        cues = context + \"_\" + syllables.lower() + \"_\" + segments\n",
    "        if \"NA_\" in cues:\n",
    "            cues = cues.replace(\"NA\", \"\")\n",
    "\n",
    "        # Append all information to the dataframe as a new row.\n",
    "        df.loc[len(df)] = {\"cues\": str(cues), \"outcomes\": str(word)}\n",
    "\n",
    "        # Save individual speaker dataframe.\n",
    "        df.to_csv(\n",
    "            \"/gpfs/project/anste145/input_files/buckeye_data/event_files/\"\n",
    "            + df_name\n",
    "            + \".tsv\"\n",
    "        )\n",
    "        speakers.append(df)\n",
    "\n",
    "# Concat all individual speaker dataframes into one dataframe.\n",
    "buckeye_event_file = pd.concat(speakers)\n",
    "buckeye_event_file.to_csv(\n",
    "    \"/gpfs/project/anste145/output/buckeye_event_file.tsv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01132adf",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# English language settings for the language parameter in the syllabifier.\n",
    "English = {\n",
    "    \"consonants\": [\n",
    "        \"B\",\n",
    "        \"CH\",\n",
    "        \"D\",\n",
    "        \"DH\",\n",
    "        \"F\",\n",
    "        \"G\",\n",
    "        \"HH\",\n",
    "        \"JH\",\n",
    "        \"K\",\n",
    "        \"L\",\n",
    "        \"M\",\n",
    "        \"N\",\n",
    "        \"NG\",\n",
    "        \"P\",\n",
    "        \"R\",\n",
    "        \"S\",\n",
    "        \"SH\",\n",
    "        \"T\",\n",
    "        \"TH\",\n",
    "        \"V\",\n",
    "        \"W\",\n",
    "        \"Y\",\n",
    "        \"Z\",\n",
    "        \"ZH\",\n",
    "    ],\n",
    "    \"vowels\": [\n",
    "        \"AA\",\n",
    "        \"AE\",\n",
    "        \"AH\",\n",
    "        \"AO\",\n",
    "        \"AW\",\n",
    "        \"AY\",\n",
    "        \"EH\",\n",
    "        \"ER\",\n",
    "        \"EY\",\n",
    "        \"IH\",\n",
    "        \"IY\",\n",
    "        \"OW\",\n",
    "        \"OY\",\n",
    "        \"UH\",\n",
    "        \"UW\",\n",
    "    ],\n",
    "    \"onsets\": [\n",
    "        \"P\",\n",
    "        \"T\",\n",
    "        \"K\",\n",
    "        \"B\",\n",
    "        \"D\",\n",
    "        \"G\",\n",
    "        \"F\",\n",
    "        \"V\",\n",
    "        \"TH\",\n",
    "        \"DH\",\n",
    "        \"S\",\n",
    "        \"Z\",\n",
    "        \"SH\",\n",
    "        \"CH\",\n",
    "        \"JH\",\n",
    "        \"M\",\n",
    "        \"N\",\n",
    "        \"R\",\n",
    "        \"L\",\n",
    "        \"HH\",\n",
    "        \"W\",\n",
    "        \"Y\",\n",
    "        \"P R\",\n",
    "        \"T R\",\n",
    "        \"K R\",\n",
    "        \"B R\",\n",
    "        \"D R\",\n",
    "        \"G R\",\n",
    "        \"F R\",\n",
    "        \"TH R\",\n",
    "        \"SH R\",\n",
    "        \"P L\",\n",
    "        \"K L\",\n",
    "        \"B L\",\n",
    "        \"G L\",\n",
    "        \"F L\",\n",
    "        \"S L\",\n",
    "        \"T W\",\n",
    "        \"K W\",\n",
    "        \"D W\",\n",
    "        \"S W\",\n",
    "        \"S P\",\n",
    "        \"S T\",\n",
    "        \"S K\",\n",
    "        \"S F\",\n",
    "        \"S M\",\n",
    "        \"S N\",\n",
    "        \"G W\",\n",
    "        \"SH W\",\n",
    "        \"S P R\",\n",
    "        \"S P L\",\n",
    "        \"S T R\",\n",
    "        \"S K R\",\n",
    "        \"S K W\",\n",
    "        \"S K L\",\n",
    "        \"TH W\",\n",
    "        \"ZH\",\n",
    "        \"P Y\",\n",
    "        \"K Y\",\n",
    "        \"B Y\",\n",
    "        \"F Y\",\n",
    "        \"HH Y\",\n",
    "        \"V Y\",\n",
    "        \"TH Y\",\n",
    "        \"M Y\",\n",
    "        \"S P Y\",\n",
    "        \"S K Y\",\n",
    "        \"G Y\",\n",
    "        \"HH W\",\n",
    "        \"\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def syllabify(language, word):\n",
    "    \"\"\"Syllabifies the word, given a language configuration loaded with\n",
    "    loadLanguage. word is either a string of phonemes from the CMU\n",
    "    pronouncing dictionary set (with optional stress numbers after vowels),\n",
    "    or a Python list of phonemes, e.g. \"B AE1 T\" or [\"B\", \"AE1\", \"T\"]\n",
    "    \"\"\"\n",
    "\n",
    "    if type(word) == str:\n",
    "        word = word.split()\n",
    "    # This is the returned data structure.\n",
    "    syllables = []\n",
    "\n",
    "    # This maintains a list of phonemes between nuclei.\n",
    "    internuclei = []\n",
    "\n",
    "    for phoneme in word:\n",
    "\n",
    "        phoneme = phoneme.strip()\n",
    "        if phoneme == \"\":\n",
    "            continue\n",
    "        stress = None\n",
    "        if phoneme[-1].isdigit():\n",
    "            stress = int(phoneme[-1])\n",
    "            phoneme = phoneme[0:-1]\n",
    "\n",
    "        # Split the consonants seen since the last nucleus into coda and\n",
    "        # onset.\n",
    "        if phoneme in language[\"vowels\"]:\n",
    "\n",
    "            coda = None\n",
    "            onset = None\n",
    "\n",
    "            # If there is a period in the input, split there.\n",
    "            if \".\" in internuclei:\n",
    "                period = internuclei.index(\".\")\n",
    "                coda = internuclei[:period]\n",
    "                onset = internuclei[period + 1 :]\n",
    "\n",
    "            else:\n",
    "                # Make the largest onset we can. The 'split' variable marks\n",
    "                # the break point.\n",
    "                for split in range(0, len(internuclei) + 1):\n",
    "                    coda = internuclei[:split]\n",
    "                    onset = internuclei[split:]\n",
    "\n",
    "                    # If we are looking at a valid onset, or if we're at the\n",
    "                    # start of the word (in which case an invalid onset is\n",
    "                    # better than a coda that doesn't follow a nucleus), or\n",
    "                    # if we've gone through all of the onsets and we didn't\n",
    "                    # find any that are valid, then split the nonvowels\n",
    "                    # we've seen at this location.\n",
    "                    if (\n",
    "                        \" \".join(onset) in language[\"onsets\"]\n",
    "                        or len(syllables) == 0\n",
    "                        or len(onset) == 0\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "            # Tack the coda onto the coda of the last syllable. Can't do it\n",
    "            # if this is the first syllable.\n",
    "            if len(syllables) > 0:\n",
    "                syllables[-1][3].extend(coda)\n",
    "\n",
    "            # Make a new syllable out of the onset and nucleus.\n",
    "            syllables.append((stress, onset, [phoneme], []))\n",
    "\n",
    "            # At this point we've processed the internuclei list.\n",
    "            internuclei = []\n",
    "\n",
    "        elif not phoneme in language[\"consonants\"] and phoneme != \".\":\n",
    "            raise ValueError(\"Invalid phoneme: \" + phoneme)\n",
    "\n",
    "        else:  # a consonant\n",
    "            internuclei.append(phoneme)\n",
    "\n",
    "    # Done looping through phonemes. We may have consonants left at the end.\n",
    "    # We may have even not found a nucleus.\n",
    "    if len(internuclei) > 0:\n",
    "        if len(syllables) == 0:\n",
    "            syllables.append((None, internuclei, [], []))\n",
    "        else:\n",
    "            syllables[-1][3].extend(internuclei)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "\n",
    "def stringify(syllables):\n",
    "    \"\"\"This function takes a syllabification returned by syllabify and\n",
    "    turns it into a string, with phonemes spearated by spaces and\n",
    "    syllables spearated by periods.\"\"\"\n",
    "    ret = []\n",
    "    for syl in syllables:\n",
    "        stress, onset, nucleus, coda = syl\n",
    "        if stress != None and len(nucleus) != 0:\n",
    "            nucleus[0] += str(stress)\n",
    "        ret.append(\"\".join(onset + nucleus + coda))\n",
    "    return \" \".join(ret)\n",
    "\n",
    "\n",
    "language = English"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
