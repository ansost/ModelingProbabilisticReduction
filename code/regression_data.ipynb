{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83954b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import buckeye\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from dp.phonemizer import Phonemizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "phonemizer = Phonemizer.from_checkpoint(\n",
    "    \"/home/ansost/slamndl/notebooks/files/en_us_cmudict_forward.pt\"\n",
    ")\n",
    "corpus = buckeye.corpus(\"/home/ansost/buckeye_corpus/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26c950",
   "metadata": {},
   "source": [
    "## Variables\n",
    "- speaker ID --> `speaker.name`\n",
    "- speaker age --> `speaker.age`\n",
    "- speaker gender --> `speaker.sex`\n",
    "- interviewer gender --> `speaker.interviewer`\n",
    "---\n",
    "- word token/ ID --> `word.orthography`\n",
    "- word duration --> `word.dur`\n",
    "- word POS --> `word.pos`\n",
    "- n_segments -->  `transcription = phonemizer(token, lang='en_us')`\n",
    "- n_syllables --> `syllables = stringify(syllabify(English, token))`\n",
    "---\n",
    "- local/global sr ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016684ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English language settings for the language parameter in the syllabifier.\n",
    "English = {\n",
    "    \"consonants\": [\n",
    "        \"B\",\n",
    "        \"CH\",\n",
    "        \"D\",\n",
    "        \"DH\",\n",
    "        \"F\",\n",
    "        \"G\",\n",
    "        \"HH\",\n",
    "        \"JH\",\n",
    "        \"K\",\n",
    "        \"L\",\n",
    "        \"M\",\n",
    "        \"N\",\n",
    "        \"NG\",\n",
    "        \"P\",\n",
    "        \"R\",\n",
    "        \"S\",\n",
    "        \"SH\",\n",
    "        \"T\",\n",
    "        \"TH\",\n",
    "        \"V\",\n",
    "        \"W\",\n",
    "        \"Y\",\n",
    "        \"Z\",\n",
    "        \"ZH\",\n",
    "    ],\n",
    "    \"vowels\": [\n",
    "        \"AA\",\n",
    "        \"AE\",\n",
    "        \"AH\",\n",
    "        \"AO\",\n",
    "        \"AW\",\n",
    "        \"AY\",\n",
    "        \"EH\",\n",
    "        \"ER\",\n",
    "        \"EY\",\n",
    "        \"IH\",\n",
    "        \"IY\",\n",
    "        \"OW\",\n",
    "        \"OY\",\n",
    "        \"UH\",\n",
    "        \"UW\",\n",
    "    ],\n",
    "    \"onsets\": [\n",
    "        \"P\",\n",
    "        \"T\",\n",
    "        \"K\",\n",
    "        \"B\",\n",
    "        \"D\",\n",
    "        \"G\",\n",
    "        \"F\",\n",
    "        \"V\",\n",
    "        \"TH\",\n",
    "        \"DH\",\n",
    "        \"S\",\n",
    "        \"Z\",\n",
    "        \"SH\",\n",
    "        \"CH\",\n",
    "        \"JH\",\n",
    "        \"M\",\n",
    "        \"N\",\n",
    "        \"R\",\n",
    "        \"L\",\n",
    "        \"HH\",\n",
    "        \"W\",\n",
    "        \"Y\",\n",
    "        \"P R\",\n",
    "        \"T R\",\n",
    "        \"K R\",\n",
    "        \"B R\",\n",
    "        \"D R\",\n",
    "        \"G R\",\n",
    "        \"F R\",\n",
    "        \"TH R\",\n",
    "        \"SH R\",\n",
    "        \"P L\",\n",
    "        \"K L\",\n",
    "        \"B L\",\n",
    "        \"G L\",\n",
    "        \"F L\",\n",
    "        \"S L\",\n",
    "        \"T W\",\n",
    "        \"K W\",\n",
    "        \"D W\",\n",
    "        \"S W\",\n",
    "        \"S P\",\n",
    "        \"S T\",\n",
    "        \"S K\",\n",
    "        \"S F\",\n",
    "        \"S M\",\n",
    "        \"S N\",\n",
    "        \"G W\",\n",
    "        \"SH W\",\n",
    "        \"S P R\",\n",
    "        \"S P L\",\n",
    "        \"S T R\",\n",
    "        \"S K R\",\n",
    "        \"S K W\",\n",
    "        \"S K L\",\n",
    "        \"TH W\",\n",
    "        \"ZH\",\n",
    "        \"P Y\",\n",
    "        \"K Y\",\n",
    "        \"B Y\",\n",
    "        \"F Y\",\n",
    "        \"HH Y\",\n",
    "        \"V Y\",\n",
    "        \"TH Y\",\n",
    "        \"M Y\",\n",
    "        \"S P Y\",\n",
    "        \"S K Y\",\n",
    "        \"G Y\",\n",
    "        \"HH W\",\n",
    "        \"\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def syllabify(language, word):\n",
    "    \"\"\"Syllabifies the word, given a language configuration loaded with\n",
    "    loadLanguage. word is either a string of phonemes from the CMU\n",
    "    pronouncing dictionary set (with optional stress numbers after vowels),\n",
    "    or a Python list of phonemes, e.g. \"B AE1 T\" or [\"B\", \"AE1\", \"T\"]\n",
    "    \"\"\"\n",
    "\n",
    "    if type(word) == str:\n",
    "        word = word.split()\n",
    "    # This is the returned data structure.\n",
    "    syllables = []\n",
    "\n",
    "    # This maintains a list of phonemes between nuclei.\n",
    "    internuclei = []\n",
    "\n",
    "    for phoneme in word:\n",
    "\n",
    "        phoneme = phoneme.strip()\n",
    "        if phoneme == \"\":\n",
    "            continue\n",
    "        stress = None\n",
    "        if phoneme[-1].isdigit():\n",
    "            stress = int(phoneme[-1])\n",
    "            phoneme = phoneme[0:-1]\n",
    "\n",
    "        # Split the consonants seen since the last nucleus into coda and\n",
    "        # onset.\n",
    "        if phoneme in language[\"vowels\"]:\n",
    "\n",
    "            coda = None\n",
    "            onset = None\n",
    "\n",
    "            # If there is a period in the input, split there.\n",
    "            if \".\" in internuclei:\n",
    "                period = internuclei.index(\".\")\n",
    "                coda = internuclei[:period]\n",
    "                onset = internuclei[period + 1 :]\n",
    "\n",
    "            else:\n",
    "                # Make the largest onset we can. The 'split' variable marks\n",
    "                # the break point.\n",
    "                for split in range(0, len(internuclei) + 1):\n",
    "                    coda = internuclei[:split]\n",
    "                    onset = internuclei[split:]\n",
    "\n",
    "                    # If we are looking at a valid onset, or if we're at the\n",
    "                    # start of the word (in which case an invalid onset is\n",
    "                    # better than a coda that doesn't follow a nucleus), or\n",
    "                    # if we've gone through all of the onsets and we didn't\n",
    "                    # find any that are valid, then split the nonvowels\n",
    "                    # we've seen at this location.\n",
    "                    if (\n",
    "                        \" \".join(onset) in language[\"onsets\"]\n",
    "                        or len(syllables) == 0\n",
    "                        or len(onset) == 0\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "            # Tack the coda onto the coda of the last syllable. Can't do it\n",
    "            # if this is the first syllable.\n",
    "            if len(syllables) > 0:\n",
    "                syllables[-1][3].extend(coda)\n",
    "\n",
    "            # Make a new syllable out of the onset and nucleus.\n",
    "            syllables.append((stress, onset, [phoneme], []))\n",
    "\n",
    "            # At this point we've processed the internuclei list.\n",
    "            internuclei = []\n",
    "\n",
    "        elif not phoneme in language[\"consonants\"] and phoneme != \".\":\n",
    "            raise ValueError(\"Invalid phoneme: \" + phoneme)\n",
    "\n",
    "        else:  # a consonant\n",
    "            internuclei.append(phoneme)\n",
    "\n",
    "    # Done looping through phonemes. We may have consonants left at the end.\n",
    "    # We may have even not found a nucleus.\n",
    "    if len(internuclei) > 0:\n",
    "        if len(syllables) == 0:\n",
    "            syllables.append((None, internuclei, [], []))\n",
    "        else:\n",
    "            syllables[-1][3].extend(internuclei)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "\n",
    "def stringify(syllables):\n",
    "    \"\"\"This function takes a syllabification returned by syllabify and\n",
    "    turns it into a string, with phonemes spearated by spaces and\n",
    "    syllables spearated by periods.\"\"\"\n",
    "    ret = []\n",
    "    for syl in syllables:\n",
    "        stress, onset, nucleus, coda = syl\n",
    "        if stress != None and len(nucleus) != 0:\n",
    "            nucleus[0] += str(stress)\n",
    "        ret.append(\"\".join(onset + nucleus + coda))\n",
    "    return \" \".join(ret)\n",
    "\n",
    "\n",
    "language = English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f116d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forbidden_words = [\n",
    "    \"uh\",\n",
    "    \"ah\",\n",
    "    \"um\",\n",
    "    \"mm\",\n",
    "    \"hm\",\n",
    "    \"huh\",\n",
    "    \"uh-huh\",\n",
    "    \"um-hum\",\n",
    "    \"huh-uh\",\n",
    "    \"hum-hum\",\n",
    "    \"hmm\",\n",
    "    \"hmmm\",\n",
    "    \"mh\",\n",
    "    \"mmh\",\n",
    "    \"oh\",\n",
    "]\n",
    "speakers = []\n",
    "\n",
    "for speaker in tqdm(corpus):\n",
    "    # Create new dataframe for speaker with all regression variables.\n",
    "    df_name = speaker.name + \"df\"\n",
    "    df_name = pd.DataFrame(\n",
    "        {\n",
    "            \"speakerID\": [],\n",
    "            \"speakerAge\": [],\n",
    "            \"speakerGender\": [],\n",
    "            \"interviewerGender\": [],\n",
    "            \"wordID\": [],\n",
    "            \"wordDur\": [],\n",
    "            \"wordPOS\": [],\n",
    "            \"n_segments\": [],\n",
    "            \"n_syllables\": [],\n",
    "            \"speechRate\": [],\n",
    "        }\n",
    "    )\n",
    "    for track in speaker:\n",
    "        for word in track.words:\n",
    "            if (\n",
    "                isinstance(word, buckeye.containers.Word)\n",
    "                and word.orthography not in forbidden_words\n",
    "            ):\n",
    "\n",
    "                # Get the segment count.\n",
    "                segments = phonemizer(word.orthography, lang=\"en_us\")\n",
    "                segments = re.sub(r\"[\\[\\]-]\", \" \", segments)\n",
    "                n_seg = len(segments.split())\n",
    "\n",
    "                # Get the syllable count.\n",
    "                syllables = stringify(syllabify(English, segments))\n",
    "                n_syll = len(syllables.split())\n",
    "\n",
    "                # Append all information to the dataframe as a new row.\n",
    "                df_name.loc[len(df_name)] = {\n",
    "                    \"speakerID\": speaker.name,\n",
    "                    \"speakerAge\": speaker.age,\n",
    "                    \"speakerGender\": speaker.sex,\n",
    "                    \"interviewerGender\": speaker.interviewer,\n",
    "                    \"wordID\": word.orthography,\n",
    "                    \"wordDur\": word.dur,\n",
    "                    \"wordPOS\": word.pos,\n",
    "                    \"n_segments\": n_seg,\n",
    "                    \"n_syllables\": n_syll,\n",
    "                }\n",
    "    speakers.append(df_name)\n",
    "\n",
    "# Concat all individual speaker dataframes into one dataframe.\n",
    "regression_data = pd.concat(speakers)\n",
    "regression_data.to_csv(\"/home/ansost/theImpossibleBachelorThesis/regression_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddebfce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regression_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
